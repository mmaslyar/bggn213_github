---
title: "class08"
author: "Maddie Maslyar (PID: 69042845)"
format: pdf
---

## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

## Data import

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
```

Make sure we do not include sample ID or diagnosis in further analysis

```{r}
# Save diagnosis outside of data frame
diagnosis <- as.factor(wisc.df$diagnosis)
# Remove diagnosis from data frame in new data frame
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

Q1. How many observations are in this dataset?

```{r}
ncol(wisc.data)
```

A1. here are 30 observations.

Q2. How many of the observations have a malignant diagnosis?

```{r}
sum(wisc.df$diagnosis == "M")

table(wisc.df$diagnosis)
```

A2. There are 212 observations with a malignant diagnosis.

Q3. How many variables/features in the data are suffixed with \_mean?

```{r}
#grep searches for pattern matches
length(grep("_mean", colnames(wisc.data)))
```

A3. There are 10 variables with the suffix "\_mean".

## Principal Component Analysis

The main function in base R for PCA is called `prcomp()`

Scaling data ensures that each feature contributes equally to the analysis, preventing variables with larger scales from dominating the principal components

Make sure to set `prcomp(x, scale = TRUE)`.

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

A4. 44.27% of the original variance is captured in PC1.

A5. Three PCs can describe greater than 70% of the original variance in the data.

A6. Seven PCs can describe greater than 90% of the original variance in the data.

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

A7. I am unable to interpret the plot due to the format and size of the data points.

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x, col = diagnosis , 
     xlab = "PC1", ylab = "PC2")


plot(wisc.pr$x[, -2 ], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

A8. The plots look similar. In both, the malignant and benign tumors cluster into two groups.

Make our main result figure - the "PC plot" or "score plot"

```{r}
library(ggplot2)
ggplot(wisc.pr$x, aes(PC1, PC2, col = diagnosis)) +
  geom_point()

```

The Malignant and benign tumors separate into two clusters.

```{r}
# Calculate the variance of the components
pr.var <- wisc.pr$sdev^2
head(pr.var)

# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )

## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr\$rotation\[,1\]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

```{r}
wisc.pr$rotation[,1]
```

A9. The component of the loading vector is -0.26085376.

## Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method = "complete")
```

Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust) 
abline(h = 19:20, col="red", lty=2)
```

A10: The clustering model has four clusters between height 19-20.

# Combining PCA and clustering

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
abline(h=70, col="red")
```

Get my cluster membership vector

```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)
```

```{r}
table(diagnosis)
```

Make a cross-table

```{r}
table(grps, diagnosis)
```

True positive: 179 False positive: 24 True negative: 333 False negative: 33

Sensitivity: TP/(TP+FN)

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```
Q13. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
(333+179)/(24+179+333+33)
```

A13. The new model identifies the two diagnoses accurately 89.98% of the time.

Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
(343+165)/(12 + 165 + 2 + 5 + 343 + 40 + 2)
```

A14. The hierarchical clustering models successfully predict 89.28% of diagnoses. This is relatively similar to the new model.